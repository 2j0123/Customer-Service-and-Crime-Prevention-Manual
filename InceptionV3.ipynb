{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2j0123/WASSUP_EST_FINAL_Team4/blob/SW/InceptionV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmQ8Uv9PwmYQ",
        "outputId": "566bd201-43d2-4bda-f2d7-1437a01fddde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Google Drive와 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B-aNlxj1n56",
        "outputId": "b59a7abd-692d-44b5-b631-405f28fe920f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5369 images belonging to 4 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 3s 0us/step\n",
            "Epoch 1/170\n",
            "168/168 [==============================] - 2498s 15s/step - loss: 1.5416 - accuracy: 0.3198\n",
            "Epoch 2/170\n",
            "168/168 [==============================] - 404s 2s/step - loss: 1.3103 - accuracy: 0.3762\n",
            "Epoch 3/170\n",
            "168/168 [==============================] - 402s 2s/step - loss: 1.2916 - accuracy: 0.3863\n",
            "Epoch 4/170\n",
            "168/168 [==============================] - 405s 2s/step - loss: 1.2655 - accuracy: 0.4081\n",
            "Epoch 5/170\n",
            "168/168 [==============================] - 398s 2s/step - loss: 1.2473 - accuracy: 0.4308\n",
            "Epoch 6/170\n",
            "168/168 [==============================] - 394s 2s/step - loss: 1.2456 - accuracy: 0.4267\n",
            "Epoch 7/170\n",
            "168/168 [==============================] - 397s 2s/step - loss: 1.2212 - accuracy: 0.4411\n",
            "Epoch 8/170\n",
            "168/168 [==============================] - 395s 2s/step - loss: 1.2096 - accuracy: 0.4425\n",
            "Epoch 9/170\n",
            "168/168 [==============================] - 410s 2s/step - loss: 1.2185 - accuracy: 0.4384\n",
            "Epoch 10/170\n",
            "168/168 [==============================] - 390s 2s/step - loss: 1.2037 - accuracy: 0.4535\n",
            "Epoch 11/170\n",
            "168/168 [==============================] - 395s 2s/step - loss: 1.2001 - accuracy: 0.4552\n",
            "Epoch 12/170\n",
            "168/168 [==============================] - 420s 3s/step - loss: 1.1913 - accuracy: 0.4634\n",
            "Epoch 13/170\n",
            "168/168 [==============================] - 399s 2s/step - loss: 1.1684 - accuracy: 0.4792\n",
            "Epoch 14/170\n",
            "168/168 [==============================] - 404s 2s/step - loss: 1.1827 - accuracy: 0.4660\n",
            "Epoch 15/170\n",
            "168/168 [==============================] - 409s 2s/step - loss: 1.1724 - accuracy: 0.4742\n",
            "Epoch 16/170\n",
            "168/168 [==============================] - 405s 2s/step - loss: 1.1598 - accuracy: 0.4789\n",
            "Epoch 17/170\n",
            "168/168 [==============================] - 430s 3s/step - loss: 1.1438 - accuracy: 0.4941\n",
            "Epoch 18/170\n",
            "168/168 [==============================] - 408s 2s/step - loss: 1.1686 - accuracy: 0.4774\n",
            "Epoch 19/170\n",
            "168/168 [==============================] - 407s 2s/step - loss: 1.1483 - accuracy: 0.4939\n",
            "Epoch 20/170\n",
            "168/168 [==============================] - 412s 2s/step - loss: 1.1431 - accuracy: 0.4967\n",
            "Epoch 21/170\n",
            "168/168 [==============================] - 409s 2s/step - loss: 1.1329 - accuracy: 0.4949\n",
            "Epoch 22/170\n",
            "168/168 [==============================] - 408s 2s/step - loss: 1.1314 - accuracy: 0.5042\n",
            "Epoch 23/170\n",
            "168/168 [==============================] - 409s 2s/step - loss: 1.1236 - accuracy: 0.5064\n",
            "Epoch 24/170\n",
            "168/168 [==============================] - 403s 2s/step - loss: 1.1234 - accuracy: 0.4999\n",
            "Epoch 25/170\n",
            "168/168 [==============================] - 409s 2s/step - loss: 1.1085 - accuracy: 0.5129\n",
            "Epoch 26/170\n",
            "168/168 [==============================] - 398s 2s/step - loss: 1.1144 - accuracy: 0.5079\n",
            "Epoch 27/170\n",
            "168/168 [==============================] - 393s 2s/step - loss: 1.1107 - accuracy: 0.5124\n",
            "Epoch 28/170\n",
            "168/168 [==============================] - 416s 2s/step - loss: 1.1065 - accuracy: 0.5161\n",
            "Epoch 29/170\n",
            "168/168 [==============================] - 406s 2s/step - loss: 1.1050 - accuracy: 0.5135\n",
            "Epoch 30/170\n",
            "168/168 [==============================] - 381s 2s/step - loss: 1.0830 - accuracy: 0.5319\n",
            "Epoch 31/170\n",
            "168/168 [==============================] - 377s 2s/step - loss: 1.0908 - accuracy: 0.5241\n",
            "Epoch 32/170\n",
            "168/168 [==============================] - 377s 2s/step - loss: 1.0868 - accuracy: 0.5213\n",
            "Epoch 33/170\n",
            "168/168 [==============================] - 403s 2s/step - loss: 1.0731 - accuracy: 0.5327\n",
            "Epoch 34/170\n",
            "168/168 [==============================] - 440s 3s/step - loss: 1.0868 - accuracy: 0.5241\n",
            "Epoch 35/170\n",
            "168/168 [==============================] - 430s 3s/step - loss: 1.0700 - accuracy: 0.5426\n",
            "Epoch 36/170\n",
            "168/168 [==============================] - 432s 3s/step - loss: 1.0594 - accuracy: 0.5463\n",
            "Epoch 37/170\n",
            "168/168 [==============================] - 427s 3s/step - loss: 1.0596 - accuracy: 0.5385\n",
            "Epoch 38/170\n",
            "168/168 [==============================] - 424s 3s/step - loss: 1.0598 - accuracy: 0.5409\n",
            "Epoch 39/170\n",
            "168/168 [==============================] - 424s 3s/step - loss: 1.0501 - accuracy: 0.5513\n",
            "Epoch 40/170\n",
            "168/168 [==============================] - 429s 3s/step - loss: 1.0442 - accuracy: 0.5452\n",
            "Epoch 41/170\n",
            "168/168 [==============================] - 424s 3s/step - loss: 1.0424 - accuracy: 0.5496\n",
            "Epoch 42/170\n",
            "168/168 [==============================] - 424s 3s/step - loss: 1.0463 - accuracy: 0.5467\n",
            "Epoch 43/170\n",
            "168/168 [==============================] - 427s 3s/step - loss: 1.0216 - accuracy: 0.5619\n",
            "Epoch 44/170\n",
            "168/168 [==============================] - 423s 3s/step - loss: 1.0375 - accuracy: 0.5508\n",
            "Epoch 45/170\n",
            "168/168 [==============================] - 423s 3s/step - loss: 1.0254 - accuracy: 0.5589\n",
            "Epoch 46/170\n",
            "168/168 [==============================] - 425s 3s/step - loss: 1.0193 - accuracy: 0.5623\n",
            "Epoch 47/170\n",
            "168/168 [==============================] - 425s 3s/step - loss: 1.0230 - accuracy: 0.5619\n",
            "Epoch 48/170\n",
            "168/168 [==============================] - 425s 3s/step - loss: 1.0125 - accuracy: 0.5610\n",
            "Epoch 49/170\n",
            "168/168 [==============================] - 407s 2s/step - loss: 1.0103 - accuracy: 0.5664\n",
            "Epoch 50/170\n",
            "168/168 [==============================] - 412s 2s/step - loss: 1.0149 - accuracy: 0.5625\n",
            "Epoch 51/170\n",
            "168/168 [==============================] - 416s 2s/step - loss: 1.0003 - accuracy: 0.5746\n",
            "Epoch 52/170\n",
            "168/168 [==============================] - 419s 2s/step - loss: 1.0013 - accuracy: 0.5765\n",
            "Epoch 53/170\n",
            "168/168 [==============================] - 452s 3s/step - loss: 0.9865 - accuracy: 0.5781\n",
            "Epoch 54/170\n",
            "168/168 [==============================] - 416s 2s/step - loss: 0.9855 - accuracy: 0.5802\n",
            "Epoch 55/170\n",
            "168/168 [==============================] - 410s 2s/step - loss: 0.9726 - accuracy: 0.5848\n",
            "Epoch 56/170\n",
            "168/168 [==============================] - 406s 2s/step - loss: 0.9802 - accuracy: 0.5748\n",
            "Epoch 57/170\n",
            "168/168 [==============================] - 414s 2s/step - loss: 0.9939 - accuracy: 0.5781\n",
            "Epoch 58/170\n",
            "168/168 [==============================] - 408s 2s/step - loss: 0.9797 - accuracy: 0.5876\n",
            "Epoch 59/170\n",
            "168/168 [==============================] - 402s 2s/step - loss: 0.9826 - accuracy: 0.5910\n",
            "Epoch 60/170\n",
            "168/168 [==============================] - 398s 2s/step - loss: 0.9812 - accuracy: 0.5873\n",
            "Epoch 61/170\n",
            "168/168 [==============================] - 403s 2s/step - loss: 0.9519 - accuracy: 0.5949\n",
            "Epoch 62/170\n",
            "168/168 [==============================] - 404s 2s/step - loss: 0.9548 - accuracy: 0.5927\n",
            "Epoch 63/170\n",
            "168/168 [==============================] - 408s 2s/step - loss: 0.9537 - accuracy: 0.5968\n",
            "Epoch 64/170\n",
            "168/168 [==============================] - 413s 2s/step - loss: 0.9461 - accuracy: 0.5925\n",
            "Epoch 65/170\n",
            "168/168 [==============================] - 412s 2s/step - loss: 0.9443 - accuracy: 0.6072\n",
            "Epoch 66/170\n",
            "168/168 [==============================] - 404s 2s/step - loss: 0.9464 - accuracy: 0.5964\n",
            "Epoch 67/170\n",
            "168/168 [==============================] - 410s 2s/step - loss: 0.9392 - accuracy: 0.6100\n",
            "Epoch 68/170\n",
            "168/168 [==============================] - 405s 2s/step - loss: 0.9289 - accuracy: 0.6053\n",
            "Epoch 69/170\n",
            "168/168 [==============================] - 408s 2s/step - loss: 0.9307 - accuracy: 0.6066\n",
            "Epoch 70/170\n",
            "168/168 [==============================] - 416s 2s/step - loss: 0.9255 - accuracy: 0.6167\n",
            "Epoch 71/170\n",
            "168/168 [==============================] - 421s 3s/step - loss: 0.9214 - accuracy: 0.6202\n",
            "Epoch 72/170\n",
            "168/168 [==============================] - 422s 3s/step - loss: 0.9262 - accuracy: 0.6130\n",
            "Epoch 73/170\n",
            "168/168 [==============================] - 435s 3s/step - loss: 0.9353 - accuracy: 0.6083\n",
            "Epoch 74/170\n",
            "168/168 [==============================] - 412s 2s/step - loss: 0.9015 - accuracy: 0.6266\n",
            "Epoch 75/170\n",
            "168/168 [==============================] - 409s 2s/step - loss: 0.9128 - accuracy: 0.6172\n",
            "Epoch 76/170\n",
            "168/168 [==============================] - 427s 3s/step - loss: 0.9160 - accuracy: 0.6247\n",
            "Epoch 77/170\n",
            "168/168 [==============================] - 410s 2s/step - loss: 0.9067 - accuracy: 0.6128\n",
            "Epoch 78/170\n",
            "168/168 [==============================] - 406s 2s/step - loss: 0.9048 - accuracy: 0.6176\n",
            "Epoch 79/170\n",
            "168/168 [==============================] - 398s 2s/step - loss: 0.9030 - accuracy: 0.6202\n",
            "Epoch 80/170\n",
            "168/168 [==============================] - 401s 2s/step - loss: 0.9064 - accuracy: 0.6260\n",
            "Epoch 81/170\n",
            "168/168 [==============================] - 402s 2s/step - loss: 0.8977 - accuracy: 0.6206\n",
            "Epoch 82/170\n",
            "168/168 [==============================] - 403s 2s/step - loss: 0.8997 - accuracy: 0.6262\n",
            "Epoch 83/170\n",
            "168/168 [==============================] - 398s 2s/step - loss: 0.9142 - accuracy: 0.6171\n",
            "Epoch 84/170\n",
            "168/168 [==============================] - 404s 2s/step - loss: 0.8789 - accuracy: 0.6400\n",
            "Epoch 85/170\n",
            "168/168 [==============================] - 404s 2s/step - loss: 0.8822 - accuracy: 0.6372\n",
            "Epoch 86/170\n",
            "168/168 [==============================] - 407s 2s/step - loss: 0.8942 - accuracy: 0.6295\n",
            "Epoch 87/170\n",
            "168/168 [==============================] - 402s 2s/step - loss: 0.8883 - accuracy: 0.6346\n",
            "Epoch 88/170\n",
            "168/168 [==============================] - 402s 2s/step - loss: 0.8881 - accuracy: 0.6275\n",
            "Epoch 89/170\n",
            "168/168 [==============================] - 403s 2s/step - loss: 0.8675 - accuracy: 0.6469\n",
            "Epoch 90/170\n",
            "168/168 [==============================] - 421s 3s/step - loss: 0.8855 - accuracy: 0.6346\n",
            "Epoch 91/170\n",
            "168/168 [==============================] - 398s 2s/step - loss: 0.8722 - accuracy: 0.6375\n",
            "Epoch 92/170\n",
            "168/168 [==============================] - 419s 2s/step - loss: 0.8576 - accuracy: 0.6493\n",
            "Epoch 93/170\n",
            "168/168 [==============================] - 400s 2s/step - loss: 0.8451 - accuracy: 0.6448\n",
            "Epoch 94/170\n",
            "168/168 [==============================] - 396s 2s/step - loss: 0.8735 - accuracy: 0.6336\n",
            "Epoch 95/170\n",
            "168/168 [==============================] - 404s 2s/step - loss: 0.8521 - accuracy: 0.6402\n",
            "Epoch 96/170\n",
            "168/168 [==============================] - 399s 2s/step - loss: 0.8823 - accuracy: 0.6314\n",
            "Epoch 97/170\n",
            "168/168 [==============================] - 399s 2s/step - loss: 0.8739 - accuracy: 0.6372\n",
            "Epoch 98/170\n",
            "168/168 [==============================] - 391s 2s/step - loss: 0.8427 - accuracy: 0.6534\n",
            "Epoch 99/170\n",
            "168/168 [==============================] - 389s 2s/step - loss: 0.8486 - accuracy: 0.6508\n",
            "Epoch 100/170\n",
            "168/168 [==============================] - 387s 2s/step - loss: 0.8663 - accuracy: 0.6389\n",
            "Epoch 101/170\n",
            "168/168 [==============================] - 387s 2s/step - loss: 0.8536 - accuracy: 0.6526\n",
            "Epoch 102/170\n",
            "168/168 [==============================] - 385s 2s/step - loss: 0.8552 - accuracy: 0.6506\n",
            "Epoch 103/170\n",
            "168/168 [==============================] - 376s 2s/step - loss: 0.8553 - accuracy: 0.6409\n",
            "Epoch 104/170\n",
            "168/168 [==============================] - 384s 2s/step - loss: 0.8480 - accuracy: 0.6562\n",
            "Epoch 105/170\n",
            "168/168 [==============================] - 385s 2s/step - loss: 0.8425 - accuracy: 0.6547\n",
            "Epoch 106/170\n",
            "168/168 [==============================] - 385s 2s/step - loss: 0.8184 - accuracy: 0.6672\n",
            "Epoch 107/170\n",
            "168/168 [==============================] - 384s 2s/step - loss: 0.8269 - accuracy: 0.6577\n",
            "Epoch 108/170\n",
            "168/168 [==============================] - 385s 2s/step - loss: 0.8260 - accuracy: 0.6716\n",
            "Epoch 109/170\n",
            "168/168 [==============================] - 386s 2s/step - loss: 0.8162 - accuracy: 0.6685\n",
            "Epoch 110/170\n",
            "168/168 [==============================] - 393s 2s/step - loss: 0.8090 - accuracy: 0.6692\n",
            "Epoch 111/170\n",
            "168/168 [==============================] - 382s 2s/step - loss: 0.8058 - accuracy: 0.6714\n",
            "Epoch 112/170\n",
            "168/168 [==============================] - 390s 2s/step - loss: 0.8277 - accuracy: 0.6629\n",
            "Epoch 113/170\n",
            "168/168 [==============================] - 377s 2s/step - loss: 0.8254 - accuracy: 0.6634\n",
            "Epoch 114/170\n",
            "168/168 [==============================] - 380s 2s/step - loss: 0.8119 - accuracy: 0.6683\n",
            "Epoch 115/170\n",
            "168/168 [==============================] - 369s 2s/step - loss: 0.8371 - accuracy: 0.6571\n",
            "Epoch 116/170\n",
            "168/168 [==============================] - 378s 2s/step - loss: 0.8088 - accuracy: 0.6705\n",
            "Epoch 117/170\n",
            "168/168 [==============================] - 385s 2s/step - loss: 0.8021 - accuracy: 0.6707\n",
            "Epoch 118/170\n",
            "168/168 [==============================] - 420s 2s/step - loss: 0.8108 - accuracy: 0.6765\n",
            "Epoch 119/170\n",
            "168/168 [==============================] - 417s 2s/step - loss: 0.8087 - accuracy: 0.6679\n",
            "Epoch 120/170\n",
            "168/168 [==============================] - 417s 2s/step - loss: 0.8102 - accuracy: 0.6685\n",
            "Epoch 121/170\n",
            "168/168 [==============================] - 418s 2s/step - loss: 0.7981 - accuracy: 0.6685\n",
            "Epoch 122/170\n",
            "168/168 [==============================] - 416s 2s/step - loss: 0.7971 - accuracy: 0.6780\n",
            "Epoch 123/170\n",
            "168/168 [==============================] - 423s 3s/step - loss: 0.8013 - accuracy: 0.6683\n",
            "Epoch 124/170\n",
            "168/168 [==============================] - 422s 3s/step - loss: 0.7998 - accuracy: 0.6668\n",
            "Epoch 125/170\n",
            "168/168 [==============================] - 423s 3s/step - loss: 0.7978 - accuracy: 0.6832\n",
            "Epoch 126/170\n",
            "168/168 [==============================] - 421s 3s/step - loss: 0.7927 - accuracy: 0.6826\n",
            "Epoch 127/170\n",
            "168/168 [==============================] - 430s 3s/step - loss: 0.7883 - accuracy: 0.6778\n",
            "Epoch 128/170\n",
            "168/168 [==============================] - 421s 3s/step - loss: 0.7921 - accuracy: 0.6737\n",
            "Epoch 129/170\n",
            "168/168 [==============================] - 424s 3s/step - loss: 0.7988 - accuracy: 0.6761\n",
            "Epoch 130/170\n",
            "168/168 [==============================] - 423s 3s/step - loss: 0.7795 - accuracy: 0.6890\n",
            "Epoch 131/170\n",
            "168/168 [==============================] - 448s 3s/step - loss: 0.7974 - accuracy: 0.6705\n",
            "Epoch 132/170\n",
            "168/168 [==============================] - 424s 3s/step - loss: 0.8289 - accuracy: 0.6573\n",
            "Epoch 133/170\n",
            "168/168 [==============================] - 424s 3s/step - loss: 0.7811 - accuracy: 0.6795\n",
            "Epoch 134/170\n",
            "168/168 [==============================] - 423s 3s/step - loss: 0.7706 - accuracy: 0.6847\n",
            "Epoch 135/170\n",
            "168/168 [==============================] - 425s 3s/step - loss: 0.7909 - accuracy: 0.6796\n",
            "Epoch 136/170\n",
            "168/168 [==============================] - 421s 3s/step - loss: 0.7803 - accuracy: 0.6841\n",
            "Epoch 137/170\n",
            "168/168 [==============================] - 430s 3s/step - loss: 0.7794 - accuracy: 0.6802\n",
            "Epoch 138/170\n",
            "168/168 [==============================] - 423s 3s/step - loss: 0.7663 - accuracy: 0.6912\n",
            "Epoch 139/170\n",
            "168/168 [==============================] - 425s 3s/step - loss: 0.7826 - accuracy: 0.6783\n",
            "Epoch 140/170\n",
            "168/168 [==============================] - 407s 2s/step - loss: 0.7590 - accuracy: 0.6890\n",
            "Epoch 141/170\n",
            "168/168 [==============================] - 406s 2s/step - loss: 0.7682 - accuracy: 0.6880\n",
            "Epoch 142/170\n",
            "168/168 [==============================] - 416s 2s/step - loss: 0.7616 - accuracy: 0.6858\n",
            "Epoch 143/170\n",
            "168/168 [==============================] - 403s 2s/step - loss: 0.7498 - accuracy: 0.6910\n",
            "Epoch 144/170\n",
            "168/168 [==============================] - 394s 2s/step - loss: 0.7770 - accuracy: 0.6845\n",
            "Epoch 145/170\n",
            "168/168 [==============================] - 391s 2s/step - loss: 0.7627 - accuracy: 0.6908\n",
            "Epoch 146/170\n",
            "168/168 [==============================] - 385s 2s/step - loss: 0.7665 - accuracy: 0.6869\n",
            "Epoch 147/170\n",
            "168/168 [==============================] - 382s 2s/step - loss: 0.7472 - accuracy: 0.7011\n",
            "Epoch 148/170\n",
            "168/168 [==============================] - 384s 2s/step - loss: 0.7483 - accuracy: 0.7074\n",
            "Epoch 149/170\n",
            "168/168 [==============================] - 381s 2s/step - loss: 0.7490 - accuracy: 0.6958\n",
            "Epoch 150/170\n",
            "168/168 [==============================] - 380s 2s/step - loss: 0.7771 - accuracy: 0.6817\n",
            "Epoch 151/170\n",
            "168/168 [==============================] - 384s 2s/step - loss: 0.7441 - accuracy: 0.6999\n",
            "Epoch 152/170\n",
            "168/168 [==============================] - 380s 2s/step - loss: 0.7597 - accuracy: 0.6936\n",
            "Epoch 153/170\n",
            "168/168 [==============================] - 396s 2s/step - loss: 0.7688 - accuracy: 0.6985\n",
            "Epoch 154/170\n",
            "168/168 [==============================] - 409s 2s/step - loss: 0.7395 - accuracy: 0.7066\n",
            "Epoch 155/170\n",
            "168/168 [==============================] - 416s 2s/step - loss: 0.7435 - accuracy: 0.7018\n",
            "Epoch 156/170\n",
            "168/168 [==============================] - 401s 2s/step - loss: 0.7504 - accuracy: 0.6936\n",
            "Epoch 157/170\n",
            "168/168 [==============================] - 393s 2s/step - loss: 0.7625 - accuracy: 0.6860\n",
            "Epoch 158/170\n",
            "168/168 [==============================] - 382s 2s/step - loss: 0.7517 - accuracy: 0.6882\n",
            "Epoch 159/170\n",
            "168/168 [==============================] - 384s 2s/step - loss: 0.7213 - accuracy: 0.7081\n",
            "Epoch 160/170\n",
            "168/168 [==============================] - 384s 2s/step - loss: 0.7208 - accuracy: 0.7173\n",
            "Epoch 161/170\n",
            "168/168 [==============================] - 382s 2s/step - loss: 0.7612 - accuracy: 0.6925\n",
            "Epoch 162/170\n",
            "168/168 [==============================] - 381s 2s/step - loss: 0.7244 - accuracy: 0.7080\n",
            "Epoch 163/170\n",
            "168/168 [==============================] - 380s 2s/step - loss: 0.7400 - accuracy: 0.6992\n",
            "Epoch 164/170\n",
            " 85/168 [==============>...............] - ETA: 3:08 - loss: 0.7261 - accuracy: 0.7092"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 import\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 데이터 경로 설정\n",
        "base_dir = '/content/drive/MyDrive/감정따뜻쟁이/train'\n",
        "\n",
        "# 이미지 데이터 생성기 설정\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# 이미지 데이터 로드\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Inception-v3 모델 불러오기 (사전 훈련된 가중치 포함)\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
        "\n",
        "# 분류기 추가\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "\n",
        "# 새로운 모델 정의 (기존의 Inception-v3 모델과 분류기를 결합)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 모든 레이어를 동결하여 기존의 가중치를 고정\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(train_generator, epochs=170)\n",
        "\n",
        "# 새로운 이미지 분류 테스트\n",
        "def classify_image(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, 0)  # 이미지 배치 추가\n",
        "    img_array /= 255.  # 이미지 정규화\n",
        "    predictions = model.predict(img_array)\n",
        "    emotion_labels = ['anger', 'happy', 'sadness', 'panic']\n",
        "    predicted_label = emotion_labels[np.argmax(predictions)]\n",
        "    return predicted_label\n",
        "\n",
        "# 예시: 이미지 분류 테스트\n",
        "test_image_path = '/content/drive/MyDrive/test_image.jpg'  # 테스트할 이미지 파일의 경로\n",
        "predicted_label = classify_image(test_image_path)\n",
        "print('Predicted emotion:', predicted_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "Dy7k21d55mjy",
        "outputId": "304ad96f-75c5-450e-c7fe-883262129e9f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e31dc15d-1194-45f9-bad3-ba3986370258\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e31dc15d-1194-45f9-bad3-ba3986370258\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving 0pnnbde1bec084d09efe5011eacbd99aa3ef8ad004d0d15ae3381adc39bd9j0qo.jpg to 0pnnbde1bec084d09efe5011eacbd99aa3ef8ad004d0d15ae3381adc39bd9j0qo.jpg\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "0pnnbde1bec084d09efe5011eacbd99aa3ef8ad004d0d15ae3381adc39bd9j0qo.jpg의 감정은 happy입니다.\n",
            "더 업로드하시겠습니까? (yes/no): yes\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-141c946b-e816-42f9-9854-c790eac4732f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-141c946b-e816-42f9-9854-c790eac4732f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving okss9bb8ceb2bbef180fbee6d54ecefdd331d67f7fe3bb167c03ddeb5d890go7q.jpg to okss9bb8ceb2bbef180fbee6d54ecefdd331d67f7fe3bb167c03ddeb5d890go7q.jpg\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "okss9bb8ceb2bbef180fbee6d54ecefdd331d67f7fe3bb167c03ddeb5d890go7q.jpg의 감정은 happy입니다.\n",
            "더 업로드하시겠습니까? (yes/no): no\n"
          ]
        }
      ],
      "source": [
        "# Google Colab에서 파일 업로드하기 위한 라이브러리 import\n",
        "from google.colab import files\n",
        "\n",
        "# 이미지 분류 함수 정의\n",
        "def classify_image(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, 0)  # 이미지 배치 추가\n",
        "    img_array /= 255.  # 이미지 정규화\n",
        "    predictions = model.predict(img_array)\n",
        "    emotion_labels = ['anger', 'happy', 'sadness', 'panic']\n",
        "    predicted_label = emotion_labels[np.argmax(predictions)]\n",
        "    return predicted_label\n",
        "\n",
        "# 사용자가 업로드한 파일을 받아서 이미지 분류하는 함수 정의\n",
        "def classify_uploaded_images():\n",
        "    uploaded = files.upload()  # 사용자로부터 파일 업로드 받기\n",
        "    for filename in uploaded.keys():\n",
        "        image_path = '/content/' + filename\n",
        "        predicted_label = classify_image(image_path)\n",
        "        print(f'{filename}의 감정은 {predicted_label}입니다.')\n",
        "\n",
        "# 사용자가 원하는 만큼 사진 또는 영상을 업로드하고 분류\n",
        "while True:\n",
        "    classify_uploaded_images()\n",
        "    continue_upload = input(\"더 업로드하시겠습니까? (yes/no): \")\n",
        "    if continue_upload.lower() != 'yes':\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 import\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from google.colab import files\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# 데이터 경로 설정 (훈련 및 검증 데이터)\n",
        "base_dir = '/content/drive/MyDrive/감정따뜻쟁이/img/train'\n",
        "validation_dir = '/content/drive/MyDrive/감정따뜻쟁이/img/val'\n",
        "\n",
        "# 이미지 데이터 생성기 설정 (훈련 및 검증 데이터)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# 이미지 데이터 로드 (훈련 및 검증 데이터)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Inception-v3 모델 불러오기 (사전 훈련된 가중치 포함)\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
        "\n",
        "# 분류기 추가\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "\n",
        "# 새로운 모델 정의 (기존의 Inception-v3 모델과 분류기를 결합)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 모든 레이어를 동결하여 기존의 가중치를 고정\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(train_generator, epochs=170, validation_data=validation_generator)\n",
        "\n",
        "# 검증 데이터에 대한 예측\n",
        "validation_generator.reset()\n",
        "y_pred = model.predict(validation_generator)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = validation_generator.classes\n",
        "\n",
        "# 성능 지표 출력\n",
        "target_names = list(validation_generator.class_indices.keys())\n",
        "print(classification_report(y_true, y_pred_classes, target_names=target_names))\n",
        "\n",
        "# 혼동 행렬 출력\n",
        "print(confusion_matrix(y_true, y_pred_classes))\n",
        "\n",
        "# 새로운 이미지 분류 테스트 함수\n",
        "def classify_image(image_path):\n",
        "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, 0)  # 이미지 배치 추가\n",
        "    img_array /= 255.  # 이미지 정규화\n",
        "    predictions = model.predict(img_array)\n",
        "    emotion_labels = ['anger', 'happy', 'sadness', 'panic']\n",
        "    predicted_label = emotion_labels[np.argmax(predictions)]\n",
        "    return predicted_label\n",
        "\n",
        "# 사용자가 업로드한 파일을 받아서 이미지 분류하는 함수 정의\n",
        "def classify_uploaded_images():\n",
        "    uploaded = files.upload()  # 사용자로부터 파일 업로드 받기\n",
        "    for filename in uploaded.keys():\n",
        "        image_path = '/content/' + filename\n",
        "        predicted_label = classify_image(image_path)\n",
        "        print(f'{filename}의 감정은 {predicted_label}입니다.')\n",
        "\n",
        "# 사용자가 원하는 만큼 사진 또는 영상을 업로드하고 분류\n",
        "while True:\n",
        "    classify_uploaded_images()\n",
        "    continue_upload = input(\"더 업로드하시겠습니까? (yes/no): \")\n",
        "    if continue_upload.lower() != 'yes':\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7CFUotyegC_T",
        "outputId": "a925a731-7082-4168-e1aa-d1bdfc90cc47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5366 images belonging to 4 classes.\n",
            "Found 1049 images belonging to 5 classes.\n",
            "Epoch 1/170\n",
            "168/168 [==============================] - ETA: 0s - loss: 1.5422 - accuracy: 0.3189"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node categorical_crossentropy/softmax_cross_entropy_with_logits defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-4-59453ffe75f5>\", line 67, in <cell line: 67>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1856, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2296, in evaluate\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 4108, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2066, in test_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2049, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2037, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1919, in test_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5579, in categorical_crossentropy\n\nlogits and labels must be broadcastable: logits_size=[32,4] labels_size=[32,5]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_test_function_30257]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-59453ffe75f5>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# 모델 훈련\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m170\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# 검증 데이터에 대한 예측\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node categorical_crossentropy/softmax_cross_entropy_with_logits defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-4-59453ffe75f5>\", line 67, in <cell line: 67>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1856, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2296, in evaluate\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 4108, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2066, in test_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2049, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2037, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1919, in test_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5579, in categorical_crossentropy\n\nlogits and labels must be broadcastable: logits_size=[32,4] labels_size=[32,5]\n\t [[{{node categorical_crossentropy/softmax_cross_entropy_with_logits}}]] [Op:__inference_test_function_30257]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 import\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "import os\n",
        "\n",
        "# 데이터 경로 설정 (훈련 및 검증 데이터)\n",
        "base_dir = '/content/drive/MyDrive/감정따뜻쟁이/img/train'\n",
        "validation_dir = '/content/drive/MyDrive/감정따뜻쟁이/img/val'\n",
        "test_dir = '/content/drive/MyDrive/감정따뜻쟁이/TEST_DATA_SET/TEST_DATA_SET'\n",
        "\n",
        "# 이미지 데이터 생성기 설정 (훈련 및 검증 데이터)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# 이미지 데이터 로드 (훈련 및 검증 데이터)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    base_dir,\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(299, 299),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Inception-v3 모델 불러오기 (사전 훈련된 가중치 포함)\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
        "\n",
        "# 분류기 추가\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "\n",
        "# 새로운 모델 정의 (기존의 Inception-v3 모델과 분류기를 결합)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# 모든 레이어를 동결하여 기존의 가중치를 고정\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 훈련\n",
        "model.fit(train_generator, epochs=170, validation_data=validation_generator)\n",
        "\n",
        "# 검증 데이터에 대한 예측\n",
        "validation_generator.reset()\n",
        "y_val_pred = model.predict(validation_generator)\n",
        "y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
        "y_val_true = validation_generator.classes\n",
        "\n",
        "# 성능 지표 출력 (검증 데이터)\n",
        "val_target_names = list(validation_generator.class_indices.keys())\n",
        "print(\"Validation Data Performance:\")\n",
        "print(classification_report(y_val_true, y_val_pred_classes, target_names=val_target_names))\n",
        "print(confusion_matrix(y_val_true, y_val_pred_classes))\n",
        "\n",
        "# 테스트 데이터에 대한 예측\n",
        "test_generator.reset()\n",
        "y_test_pred = model.predict(test_generator)\n",
        "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
        "y_test_true = test_generator.classes\n",
        "\n",
        "# 성능 지표 출력 (테스트 데이터)\n",
        "test_target_names = list(test_generator.class_indices.keys())\n",
        "print(\"Test Data Performance:\")\n",
        "print(classification_report(y_test_true, y_test_pred_classes, target_names=test_target_names))\n",
        "print(confusion_matrix(y_test_true, y_test_pred_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRGBgIkKLK6G",
        "outputId": "85321b2e-d2ba-4b47-cde9-bee7d52c0f7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5366 images belonging to 4 classes.\n",
            "Found 1049 images belonging to 4 classes.\n",
            "Found 1013 images belonging to 4 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 3s 0us/step\n",
            "Epoch 1/170\n",
            "168/168 [==============================] - 3150s 18s/step - loss: 1.5252 - accuracy: 0.3185 - val_loss: 1.3440 - val_accuracy: 0.2974\n",
            "Epoch 2/170\n",
            "168/168 [==============================] - 466s 3s/step - loss: 1.3001 - accuracy: 0.3848 - val_loss: 1.3137 - val_accuracy: 0.3775\n",
            "Epoch 3/170\n",
            "168/168 [==============================] - 461s 3s/step - loss: 1.2855 - accuracy: 0.3899 - val_loss: 1.3008 - val_accuracy: 0.3651\n",
            "Epoch 4/170\n",
            "168/168 [==============================] - 459s 3s/step - loss: 1.2558 - accuracy: 0.4133 - val_loss: 1.3354 - val_accuracy: 0.3613\n",
            "Epoch 5/170\n",
            "168/168 [==============================] - 462s 3s/step - loss: 1.2568 - accuracy: 0.4193 - val_loss: 1.2820 - val_accuracy: 0.4051\n",
            "Epoch 6/170\n",
            "168/168 [==============================] - 466s 3s/step - loss: 1.2418 - accuracy: 0.4320 - val_loss: 1.3342 - val_accuracy: 0.3947\n",
            "Epoch 7/170\n",
            "168/168 [==============================] - 466s 3s/step - loss: 1.2220 - accuracy: 0.4420 - val_loss: 1.3241 - val_accuracy: 0.3632\n",
            "Epoch 8/170\n",
            "168/168 [==============================] - 465s 3s/step - loss: 1.2183 - accuracy: 0.4480 - val_loss: 1.3517 - val_accuracy: 0.3327\n",
            "Epoch 9/170\n",
            "168/168 [==============================] - 458s 3s/step - loss: 1.2074 - accuracy: 0.4618 - val_loss: 1.3244 - val_accuracy: 0.3537\n",
            "Epoch 10/170\n",
            "168/168 [==============================] - 457s 3s/step - loss: 1.1938 - accuracy: 0.4558 - val_loss: 1.2940 - val_accuracy: 0.3651\n",
            "Epoch 11/170\n",
            "168/168 [==============================] - 457s 3s/step - loss: 1.1932 - accuracy: 0.4618 - val_loss: 1.3061 - val_accuracy: 0.3603\n",
            "Epoch 12/170\n",
            "168/168 [==============================] - 457s 3s/step - loss: 1.1775 - accuracy: 0.4761 - val_loss: 1.2697 - val_accuracy: 0.3994\n",
            "Epoch 13/170\n",
            "168/168 [==============================] - 468s 3s/step - loss: 1.1718 - accuracy: 0.4773 - val_loss: 1.3594 - val_accuracy: 0.3508\n",
            "Epoch 14/170\n",
            "168/168 [==============================] - 474s 3s/step - loss: 1.1637 - accuracy: 0.4855 - val_loss: 1.3037 - val_accuracy: 0.4175\n",
            "Epoch 15/170\n",
            "168/168 [==============================] - 468s 3s/step - loss: 1.1628 - accuracy: 0.4870 - val_loss: 1.3018 - val_accuracy: 0.3908\n",
            "Epoch 16/170\n",
            "168/168 [==============================] - 471s 3s/step - loss: 1.1608 - accuracy: 0.4883 - val_loss: 1.2991 - val_accuracy: 0.4004\n",
            "Epoch 17/170\n",
            "168/168 [==============================] - 479s 3s/step - loss: 1.1548 - accuracy: 0.4898 - val_loss: 1.2873 - val_accuracy: 0.4242\n",
            "Epoch 18/170\n",
            "168/168 [==============================] - 470s 3s/step - loss: 1.1545 - accuracy: 0.4886 - val_loss: 1.3168 - val_accuracy: 0.3813\n",
            "Epoch 19/170\n",
            "168/168 [==============================] - 463s 3s/step - loss: 1.1476 - accuracy: 0.4953 - val_loss: 1.2557 - val_accuracy: 0.4080\n",
            "Epoch 20/170\n",
            "168/168 [==============================] - 464s 3s/step - loss: 1.1393 - accuracy: 0.5041 - val_loss: 1.3400 - val_accuracy: 0.3832\n",
            "Epoch 21/170\n",
            "168/168 [==============================] - 461s 3s/step - loss: 1.1404 - accuracy: 0.4953 - val_loss: 1.2898 - val_accuracy: 0.4166\n",
            "Epoch 22/170\n",
            "168/168 [==============================] - 461s 3s/step - loss: 1.1226 - accuracy: 0.4991 - val_loss: 1.2644 - val_accuracy: 0.4118\n",
            "Epoch 23/170\n",
            "168/168 [==============================] - 477s 3s/step - loss: 1.1254 - accuracy: 0.5056 - val_loss: 1.2836 - val_accuracy: 0.4023\n",
            "Epoch 24/170\n",
            "168/168 [==============================] - 464s 3s/step - loss: 1.1169 - accuracy: 0.5017 - val_loss: 1.3188 - val_accuracy: 0.4042\n",
            "Epoch 25/170\n",
            "168/168 [==============================] - 471s 3s/step - loss: 1.1197 - accuracy: 0.5050 - val_loss: 1.3933 - val_accuracy: 0.4071\n",
            "Epoch 26/170\n",
            "168/168 [==============================] - 469s 3s/step - loss: 1.1231 - accuracy: 0.4989 - val_loss: 1.2567 - val_accuracy: 0.4280\n",
            "Epoch 27/170\n",
            "168/168 [==============================] - 477s 3s/step - loss: 1.1163 - accuracy: 0.5082 - val_loss: 1.2906 - val_accuracy: 0.4299\n",
            "Epoch 28/170\n",
            "168/168 [==============================] - 475s 3s/step - loss: 1.1121 - accuracy: 0.5123 - val_loss: 1.2870 - val_accuracy: 0.3899\n",
            "Epoch 29/170\n",
            "168/168 [==============================] - 472s 3s/step - loss: 1.0976 - accuracy: 0.5166 - val_loss: 1.3357 - val_accuracy: 0.3785\n",
            "Epoch 30/170\n",
            "168/168 [==============================] - 465s 3s/step - loss: 1.0965 - accuracy: 0.5225 - val_loss: 1.2649 - val_accuracy: 0.4214\n",
            "Epoch 31/170\n",
            "168/168 [==============================] - 476s 3s/step - loss: 1.0861 - accuracy: 0.5263 - val_loss: 1.2913 - val_accuracy: 0.4109\n",
            "Epoch 32/170\n",
            "168/168 [==============================] - 472s 3s/step - loss: 1.0909 - accuracy: 0.5291 - val_loss: 1.2713 - val_accuracy: 0.4147\n",
            "Epoch 33/170\n",
            "168/168 [==============================] - 497s 3s/step - loss: 1.0827 - accuracy: 0.5259 - val_loss: 1.3175 - val_accuracy: 0.3956\n",
            "Epoch 34/170\n",
            "168/168 [==============================] - 475s 3s/step - loss: 1.0835 - accuracy: 0.5328 - val_loss: 1.2878 - val_accuracy: 0.4299\n",
            "Epoch 35/170\n",
            "168/168 [==============================] - 473s 3s/step - loss: 1.0908 - accuracy: 0.5259 - val_loss: 1.2919 - val_accuracy: 0.4080\n",
            "Epoch 36/170\n",
            "168/168 [==============================] - 465s 3s/step - loss: 1.0682 - accuracy: 0.5399 - val_loss: 1.3721 - val_accuracy: 0.4185\n",
            "Epoch 37/170\n",
            "168/168 [==============================] - 469s 3s/step - loss: 1.0623 - accuracy: 0.5339 - val_loss: 1.3676 - val_accuracy: 0.3670\n",
            "Epoch 38/170\n",
            "168/168 [==============================] - 468s 3s/step - loss: 1.0569 - accuracy: 0.5485 - val_loss: 1.2844 - val_accuracy: 0.4194\n",
            "Epoch 39/170\n",
            "168/168 [==============================] - 459s 3s/step - loss: 1.0490 - accuracy: 0.5492 - val_loss: 1.3849 - val_accuracy: 0.4109\n",
            "Epoch 40/170\n",
            "168/168 [==============================] - 460s 3s/step - loss: 1.0363 - accuracy: 0.5553 - val_loss: 1.2826 - val_accuracy: 0.4280\n",
            "Epoch 41/170\n",
            "168/168 [==============================] - 464s 3s/step - loss: 1.0378 - accuracy: 0.5527 - val_loss: 1.3225 - val_accuracy: 0.3985\n",
            "Epoch 42/170\n",
            "168/168 [==============================] - 466s 3s/step - loss: 1.0534 - accuracy: 0.5339 - val_loss: 1.2969 - val_accuracy: 0.4175\n",
            "Epoch 43/170\n",
            "168/168 [==============================] - 474s 3s/step - loss: 1.0458 - accuracy: 0.5462 - val_loss: 1.3171 - val_accuracy: 0.3975\n",
            "Epoch 44/170\n",
            "168/168 [==============================] - 467s 3s/step - loss: 1.0380 - accuracy: 0.5520 - val_loss: 1.3295 - val_accuracy: 0.4023\n",
            "Epoch 45/170\n",
            "168/168 [==============================] - 468s 3s/step - loss: 1.0399 - accuracy: 0.5561 - val_loss: 1.3182 - val_accuracy: 0.4185\n",
            "Epoch 46/170\n",
            "168/168 [==============================] - 457s 3s/step - loss: 1.0144 - accuracy: 0.5708 - val_loss: 1.3198 - val_accuracy: 0.4290\n",
            "Epoch 47/170\n",
            "168/168 [==============================] - 457s 3s/step - loss: 1.0234 - accuracy: 0.5626 - val_loss: 1.3348 - val_accuracy: 0.4013\n",
            "Epoch 48/170\n",
            "168/168 [==============================] - 488s 3s/step - loss: 1.0201 - accuracy: 0.5680 - val_loss: 1.3142 - val_accuracy: 0.4261\n",
            "Epoch 49/170\n",
            "168/168 [==============================] - 461s 3s/step - loss: 1.0214 - accuracy: 0.5736 - val_loss: 1.3105 - val_accuracy: 0.4128\n",
            "Epoch 50/170\n",
            "168/168 [==============================] - 460s 3s/step - loss: 1.0084 - accuracy: 0.5703 - val_loss: 1.3331 - val_accuracy: 0.4214\n",
            "Epoch 51/170\n",
            "168/168 [==============================] - 457s 3s/step - loss: 0.9939 - accuracy: 0.5745 - val_loss: 1.3434 - val_accuracy: 0.3947\n",
            "Epoch 52/170\n",
            "168/168 [==============================] - 451s 3s/step - loss: 1.0103 - accuracy: 0.5684 - val_loss: 1.3369 - val_accuracy: 0.4137\n",
            "Epoch 53/170\n",
            "168/168 [==============================] - 478s 3s/step - loss: 1.0083 - accuracy: 0.5671 - val_loss: 1.3479 - val_accuracy: 0.3918\n",
            "Epoch 54/170\n",
            "168/168 [==============================] - 454s 3s/step - loss: 0.9747 - accuracy: 0.5867 - val_loss: 1.4180 - val_accuracy: 0.3861\n",
            "Epoch 55/170\n",
            "168/168 [==============================] - 461s 3s/step - loss: 1.0143 - accuracy: 0.5624 - val_loss: 1.3258 - val_accuracy: 0.4071\n",
            "Epoch 56/170\n",
            "168/168 [==============================] - 480s 3s/step - loss: 0.9878 - accuracy: 0.5891 - val_loss: 1.3199 - val_accuracy: 0.4166\n",
            "Epoch 57/170\n",
            "168/168 [==============================] - 477s 3s/step - loss: 0.9757 - accuracy: 0.5848 - val_loss: 1.3680 - val_accuracy: 0.3928\n",
            "Epoch 58/170\n",
            "168/168 [==============================] - 472s 3s/step - loss: 0.9680 - accuracy: 0.5883 - val_loss: 1.3503 - val_accuracy: 0.4042\n",
            "Epoch 59/170\n",
            "168/168 [==============================] - 470s 3s/step - loss: 0.9830 - accuracy: 0.5854 - val_loss: 1.3384 - val_accuracy: 0.4318\n",
            "Epoch 60/170\n",
            "168/168 [==============================] - 471s 3s/step - loss: 0.9786 - accuracy: 0.5859 - val_loss: 1.3705 - val_accuracy: 0.4109\n",
            "Epoch 61/170\n",
            "168/168 [==============================] - 483s 3s/step - loss: 0.9565 - accuracy: 0.5965 - val_loss: 1.3614 - val_accuracy: 0.4309\n",
            "Epoch 62/170\n",
            "168/168 [==============================] - 463s 3s/step - loss: 0.9667 - accuracy: 0.5915 - val_loss: 1.3830 - val_accuracy: 0.3966\n",
            "Epoch 63/170\n",
            "168/168 [==============================] - 464s 3s/step - loss: 0.9458 - accuracy: 0.5962 - val_loss: 1.3959 - val_accuracy: 0.4156\n",
            "Epoch 64/170\n",
            "168/168 [==============================] - 469s 3s/step - loss: 0.9865 - accuracy: 0.5798 - val_loss: 1.3689 - val_accuracy: 0.4051\n",
            "Epoch 65/170\n",
            "168/168 [==============================] - 466s 3s/step - loss: 0.9605 - accuracy: 0.5963 - val_loss: 1.3781 - val_accuracy: 0.4118\n",
            "Epoch 66/170\n",
            "168/168 [==============================] - 468s 3s/step - loss: 0.9499 - accuracy: 0.6012 - val_loss: 1.3860 - val_accuracy: 0.4147\n",
            "Epoch 67/170\n",
            "168/168 [==============================] - 467s 3s/step - loss: 0.9496 - accuracy: 0.5941 - val_loss: 1.4509 - val_accuracy: 0.4118\n",
            "Epoch 68/170\n",
            "168/168 [==============================] - 475s 3s/step - loss: 0.9558 - accuracy: 0.6045 - val_loss: 1.3397 - val_accuracy: 0.3975\n",
            "Epoch 69/170\n",
            "168/168 [==============================] - 469s 3s/step - loss: 0.9646 - accuracy: 0.6031 - val_loss: 1.3812 - val_accuracy: 0.4032\n",
            "Epoch 70/170\n",
            "168/168 [==============================] - 470s 3s/step - loss: 0.9358 - accuracy: 0.6161 - val_loss: 1.4047 - val_accuracy: 0.4051\n",
            "Epoch 71/170\n",
            "168/168 [==============================] - 463s 3s/step - loss: 0.9512 - accuracy: 0.6055 - val_loss: 1.4467 - val_accuracy: 0.3928\n",
            "Epoch 72/170\n",
            "168/168 [==============================] - 472s 3s/step - loss: 0.9396 - accuracy: 0.6116 - val_loss: 1.3950 - val_accuracy: 0.3947\n",
            "Epoch 73/170\n",
            "168/168 [==============================] - 473s 3s/step - loss: 0.9204 - accuracy: 0.6142 - val_loss: 1.4101 - val_accuracy: 0.3870\n",
            "Epoch 74/170\n",
            "168/168 [==============================] - 459s 3s/step - loss: 0.9272 - accuracy: 0.6098 - val_loss: 1.4112 - val_accuracy: 0.3861\n",
            "Epoch 75/170\n",
            "168/168 [==============================] - 456s 3s/step - loss: 0.9230 - accuracy: 0.6187 - val_loss: 1.4218 - val_accuracy: 0.4023\n",
            "Epoch 76/170\n",
            "168/168 [==============================] - 464s 3s/step - loss: 0.9200 - accuracy: 0.6146 - val_loss: 1.3550 - val_accuracy: 0.4204\n",
            "Epoch 77/170\n",
            "168/168 [==============================] - 465s 3s/step - loss: 0.9234 - accuracy: 0.6122 - val_loss: 1.4058 - val_accuracy: 0.3985\n",
            "Epoch 78/170\n",
            "168/168 [==============================] - 497s 3s/step - loss: 0.9010 - accuracy: 0.6278 - val_loss: 1.4065 - val_accuracy: 0.4032\n",
            "Epoch 79/170\n",
            "168/168 [==============================] - 469s 3s/step - loss: 0.9299 - accuracy: 0.6072 - val_loss: 1.4315 - val_accuracy: 0.3956\n",
            "Epoch 80/170\n",
            "168/168 [==============================] - 464s 3s/step - loss: 0.9142 - accuracy: 0.6170 - val_loss: 1.4535 - val_accuracy: 0.4175\n",
            "Epoch 81/170\n",
            "168/168 [==============================] - 468s 3s/step - loss: 0.9151 - accuracy: 0.6167 - val_loss: 1.4168 - val_accuracy: 0.4128\n",
            "Epoch 82/170\n",
            "168/168 [==============================] - 467s 3s/step - loss: 0.9015 - accuracy: 0.6340 - val_loss: 1.5774 - val_accuracy: 0.3947\n",
            "Epoch 83/170\n",
            "168/168 [==============================] - 484s 3s/step - loss: 0.9063 - accuracy: 0.6237 - val_loss: 1.4502 - val_accuracy: 0.4090\n",
            "Epoch 84/170\n",
            "168/168 [==============================] - 469s 3s/step - loss: 0.9076 - accuracy: 0.6211 - val_loss: 1.4216 - val_accuracy: 0.4023\n",
            "Epoch 85/170\n",
            "168/168 [==============================] - 467s 3s/step - loss: 0.9192 - accuracy: 0.6096 - val_loss: 1.5079 - val_accuracy: 0.3718\n",
            "Epoch 86/170\n",
            "168/168 [==============================] - 471s 3s/step - loss: 0.9038 - accuracy: 0.6232 - val_loss: 1.4188 - val_accuracy: 0.4090\n",
            "Epoch 87/170\n",
            "168/168 [==============================] - 470s 3s/step - loss: 0.8919 - accuracy: 0.6193 - val_loss: 1.4507 - val_accuracy: 0.3880\n",
            "Epoch 88/170\n",
            "168/168 [==============================] - 471s 3s/step - loss: 0.8808 - accuracy: 0.6381 - val_loss: 1.4062 - val_accuracy: 0.4109\n",
            "Epoch 89/170\n",
            "168/168 [==============================] - 471s 3s/step - loss: 0.8996 - accuracy: 0.6262 - val_loss: 1.4536 - val_accuracy: 0.4061\n",
            "Epoch 90/170\n",
            "168/168 [==============================] - 464s 3s/step - loss: 0.8927 - accuracy: 0.6264 - val_loss: 1.4010 - val_accuracy: 0.4032\n",
            "Epoch 91/170\n",
            "168/168 [==============================] - 461s 3s/step - loss: 0.8767 - accuracy: 0.6426 - val_loss: 1.4944 - val_accuracy: 0.3899\n",
            "Epoch 92/170\n",
            "168/168 [==============================] - 464s 3s/step - loss: 0.8686 - accuracy: 0.6359 - val_loss: 1.4520 - val_accuracy: 0.3861\n",
            "Epoch 93/170\n",
            "168/168 [==============================] - 472s 3s/step - loss: 0.8778 - accuracy: 0.6400 - val_loss: 1.4293 - val_accuracy: 0.4118\n",
            "Epoch 94/170\n",
            "168/168 [==============================] - 464s 3s/step - loss: 0.8920 - accuracy: 0.6230 - val_loss: 1.4770 - val_accuracy: 0.3880\n",
            "Epoch 95/170\n",
            "168/168 [==============================] - 463s 3s/step - loss: 0.8721 - accuracy: 0.6405 - val_loss: 1.4869 - val_accuracy: 0.4128\n",
            "Epoch 96/170\n",
            "168/168 [==============================] - 475s 3s/step - loss: 0.8747 - accuracy: 0.6400 - val_loss: 1.4227 - val_accuracy: 0.4204\n",
            "Epoch 97/170\n",
            "168/168 [==============================] - 474s 3s/step - loss: 0.8777 - accuracy: 0.6340 - val_loss: 1.4654 - val_accuracy: 0.4137\n",
            "Epoch 98/170\n",
            "168/168 [==============================] - 479s 3s/step - loss: 0.8811 - accuracy: 0.6318 - val_loss: 1.4489 - val_accuracy: 0.3994\n",
            "Epoch 99/170\n",
            "168/168 [==============================] - 473s 3s/step - loss: 0.8546 - accuracy: 0.6470 - val_loss: 1.4660 - val_accuracy: 0.3765\n",
            "Epoch 100/170\n",
            "168/168 [==============================] - 486s 3s/step - loss: 0.8642 - accuracy: 0.6429 - val_loss: 1.4359 - val_accuracy: 0.4194\n",
            "Epoch 101/170\n",
            "168/168 [==============================] - 471s 3s/step - loss: 0.8651 - accuracy: 0.6431 - val_loss: 1.4891 - val_accuracy: 0.3804\n",
            "Epoch 102/170\n",
            "168/168 [==============================] - 478s 3s/step - loss: 0.8585 - accuracy: 0.6431 - val_loss: 1.4865 - val_accuracy: 0.4061\n",
            "Epoch 103/170\n",
            "168/168 [==============================] - 487s 3s/step - loss: 0.8617 - accuracy: 0.6457 - val_loss: 1.5132 - val_accuracy: 0.3889\n",
            "Epoch 104/170\n",
            "168/168 [==============================] - 486s 3s/step - loss: 0.8634 - accuracy: 0.6437 - val_loss: 1.5143 - val_accuracy: 0.3918\n",
            "Epoch 105/170\n",
            "168/168 [==============================] - 471s 3s/step - loss: 0.8522 - accuracy: 0.6472 - val_loss: 1.4944 - val_accuracy: 0.3851\n",
            "Epoch 106/170\n",
            "168/168 [==============================] - 467s 3s/step - loss: 0.8381 - accuracy: 0.6537 - val_loss: 1.5451 - val_accuracy: 0.3765\n",
            "Epoch 107/170\n",
            "168/168 [==============================] - 470s 3s/step - loss: 0.8463 - accuracy: 0.6523 - val_loss: 1.4952 - val_accuracy: 0.3851\n",
            "Epoch 108/170\n",
            "168/168 [==============================] - 469s 3s/step - loss: 0.8569 - accuracy: 0.6491 - val_loss: 1.5452 - val_accuracy: 0.4051\n",
            "Epoch 109/170\n",
            "168/168 [==============================] - 465s 3s/step - loss: 0.8637 - accuracy: 0.6513 - val_loss: 1.4795 - val_accuracy: 0.3861\n",
            "Epoch 110/170\n",
            "168/168 [==============================] - 463s 3s/step - loss: 0.8605 - accuracy: 0.6470 - val_loss: 1.5600 - val_accuracy: 0.4137\n",
            "Epoch 111/170\n",
            "168/168 [==============================] - 467s 3s/step - loss: 0.8404 - accuracy: 0.6582 - val_loss: 1.5217 - val_accuracy: 0.3908\n",
            "Epoch 112/170\n",
            "168/168 [==============================] - 468s 3s/step - loss: 0.8540 - accuracy: 0.6571 - val_loss: 1.4935 - val_accuracy: 0.3889\n",
            "Epoch 113/170\n",
            "168/168 [==============================] - 469s 3s/step - loss: 0.8469 - accuracy: 0.6502 - val_loss: 1.5429 - val_accuracy: 0.3985\n",
            "Epoch 114/170\n",
            "168/168 [==============================] - 464s 3s/step - loss: 0.8415 - accuracy: 0.6554 - val_loss: 1.4978 - val_accuracy: 0.4175\n",
            "Epoch 115/170\n",
            "168/168 [==============================] - 463s 3s/step - loss: 0.8130 - accuracy: 0.6690 - val_loss: 1.5211 - val_accuracy: 0.4223\n",
            "Epoch 116/170\n",
            "168/168 [==============================] - 460s 3s/step - loss: 0.8397 - accuracy: 0.6556 - val_loss: 1.5230 - val_accuracy: 0.3775\n",
            "Epoch 117/170\n",
            "168/168 [==============================] - 463s 3s/step - loss: 0.8151 - accuracy: 0.6701 - val_loss: 1.5644 - val_accuracy: 0.4128\n",
            "Epoch 118/170\n",
            "168/168 [==============================] - 489s 3s/step - loss: 0.8268 - accuracy: 0.6549 - val_loss: 1.6405 - val_accuracy: 0.3851\n",
            "Epoch 119/170\n",
            "168/168 [==============================] - 464s 3s/step - loss: 0.8366 - accuracy: 0.6569 - val_loss: 1.5303 - val_accuracy: 0.4128\n",
            "Epoch 120/170\n",
            "168/168 [==============================] - 460s 3s/step - loss: 0.8360 - accuracy: 0.6549 - val_loss: 1.5748 - val_accuracy: 0.3737\n",
            "Epoch 121/170\n",
            "168/168 [==============================] - 461s 3s/step - loss: 0.8426 - accuracy: 0.6517 - val_loss: 1.5617 - val_accuracy: 0.3775\n",
            "Epoch 122/170\n",
            "168/168 [==============================] - 463s 3s/step - loss: 0.8337 - accuracy: 0.6573 - val_loss: 1.5667 - val_accuracy: 0.3918\n",
            "Epoch 123/170\n",
            "168/168 [==============================] - 468s 3s/step - loss: 0.8185 - accuracy: 0.6701 - val_loss: 1.5094 - val_accuracy: 0.3832\n",
            "Epoch 124/170\n",
            "168/168 [==============================] - 464s 3s/step - loss: 0.8167 - accuracy: 0.6646 - val_loss: 1.5287 - val_accuracy: 0.3937\n",
            "Epoch 125/170\n",
            "168/168 [==============================] - 465s 3s/step - loss: 0.8349 - accuracy: 0.6588 - val_loss: 1.5383 - val_accuracy: 0.4099\n",
            "Epoch 126/170\n",
            "168/168 [==============================] - 465s 3s/step - loss: 0.8071 - accuracy: 0.6675 - val_loss: 1.5473 - val_accuracy: 0.4128\n",
            "Epoch 127/170\n",
            "168/168 [==============================] - 471s 3s/step - loss: 0.8103 - accuracy: 0.6660 - val_loss: 1.5812 - val_accuracy: 0.3908\n",
            "Epoch 128/170\n",
            "168/168 [==============================] - 474s 3s/step - loss: 0.8073 - accuracy: 0.6677 - val_loss: 1.5864 - val_accuracy: 0.3937\n",
            "Epoch 129/170\n",
            "168/168 [==============================] - 467s 3s/step - loss: 0.7948 - accuracy: 0.6783 - val_loss: 1.5164 - val_accuracy: 0.4147\n",
            "Epoch 130/170\n",
            "168/168 [==============================] - 468s 3s/step - loss: 0.8186 - accuracy: 0.6653 - val_loss: 1.5201 - val_accuracy: 0.3994\n",
            "Epoch 131/170\n",
            "168/168 [==============================] - 469s 3s/step - loss: 0.7992 - accuracy: 0.6718 - val_loss: 1.5427 - val_accuracy: 0.4051\n",
            "Epoch 132/170\n",
            "168/168 [==============================] - 470s 3s/step - loss: 0.8068 - accuracy: 0.6642 - val_loss: 1.5527 - val_accuracy: 0.4013\n",
            "Epoch 133/170\n",
            "168/168 [==============================] - 492s 3s/step - loss: 0.8062 - accuracy: 0.6679 - val_loss: 1.5955 - val_accuracy: 0.4004\n",
            "Epoch 134/170\n",
            "168/168 [==============================] - 471s 3s/step - loss: 0.8108 - accuracy: 0.6716 - val_loss: 1.5833 - val_accuracy: 0.3975\n",
            "Epoch 135/170\n",
            "168/168 [==============================] - 469s 3s/step - loss: 0.8228 - accuracy: 0.6633 - val_loss: 1.5150 - val_accuracy: 0.4137\n",
            "Epoch 136/170\n",
            "168/168 [==============================] - 466s 3s/step - loss: 0.7962 - accuracy: 0.6793 - val_loss: 1.5262 - val_accuracy: 0.3899\n",
            "Epoch 137/170\n",
            "168/168 [==============================] - 470s 3s/step - loss: 0.7996 - accuracy: 0.6726 - val_loss: 1.5652 - val_accuracy: 0.4109\n",
            "Epoch 138/170\n",
            "168/168 [==============================] - 474s 3s/step - loss: 0.7786 - accuracy: 0.6810 - val_loss: 1.5623 - val_accuracy: 0.4013\n",
            "Epoch 139/170\n",
            "168/168 [==============================] - 468s 3s/step - loss: 0.8110 - accuracy: 0.6588 - val_loss: 1.5429 - val_accuracy: 0.4175\n",
            "Epoch 140/170\n",
            "168/168 [==============================] - 467s 3s/step - loss: 0.8022 - accuracy: 0.6759 - val_loss: 1.6195 - val_accuracy: 0.3842\n",
            "Epoch 141/170\n",
            "168/168 [==============================] - 467s 3s/step - loss: 0.7899 - accuracy: 0.6787 - val_loss: 1.5835 - val_accuracy: 0.3947\n",
            "Epoch 142/170\n",
            "168/168 [==============================] - 489s 3s/step - loss: 0.7920 - accuracy: 0.6810 - val_loss: 1.6722 - val_accuracy: 0.3861\n",
            "Epoch 143/170\n",
            "168/168 [==============================] - 473s 3s/step - loss: 0.7796 - accuracy: 0.6858 - val_loss: 1.5446 - val_accuracy: 0.4118\n",
            "Epoch 144/170\n",
            "168/168 [==============================] - 470s 3s/step - loss: 0.7940 - accuracy: 0.6772 - val_loss: 1.6352 - val_accuracy: 0.3889\n",
            "Epoch 145/170\n",
            "168/168 [==============================] - 471s 3s/step - loss: 0.8018 - accuracy: 0.6679 - val_loss: 1.5662 - val_accuracy: 0.3947\n",
            "Epoch 146/170\n",
            "168/168 [==============================] - 473s 3s/step - loss: 0.8080 - accuracy: 0.6683 - val_loss: 1.6131 - val_accuracy: 0.4233\n",
            "Epoch 147/170\n",
            "168/168 [==============================] - 471s 3s/step - loss: 0.8005 - accuracy: 0.6731 - val_loss: 1.6584 - val_accuracy: 0.3775\n",
            "Epoch 148/170\n",
            "168/168 [==============================] - 479s 3s/step - loss: 0.7864 - accuracy: 0.6780 - val_loss: 1.5912 - val_accuracy: 0.3880\n",
            "Epoch 149/170\n",
            "168/168 [==============================] - 470s 3s/step - loss: 0.7902 - accuracy: 0.6761 - val_loss: 1.6023 - val_accuracy: 0.3842\n",
            "Epoch 150/170\n",
            "168/168 [==============================] - 470s 3s/step - loss: 0.7855 - accuracy: 0.6688 - val_loss: 1.5713 - val_accuracy: 0.3899\n",
            "Epoch 151/170\n",
            "168/168 [==============================] - 469s 3s/step - loss: 0.7745 - accuracy: 0.6869 - val_loss: 1.5782 - val_accuracy: 0.4118\n",
            "Epoch 152/170\n",
            "168/168 [==============================] - 467s 3s/step - loss: 0.7739 - accuracy: 0.6893 - val_loss: 1.5437 - val_accuracy: 0.4194\n",
            "Epoch 153/170\n",
            "168/168 [==============================] - 473s 3s/step - loss: 0.7785 - accuracy: 0.6856 - val_loss: 1.5906 - val_accuracy: 0.4194\n",
            "Epoch 154/170\n",
            "168/168 [==============================] - 468s 3s/step - loss: 0.7833 - accuracy: 0.6836 - val_loss: 1.6280 - val_accuracy: 0.4023\n",
            "Epoch 155/170\n",
            "168/168 [==============================] - 475s 3s/step - loss: 0.7731 - accuracy: 0.6882 - val_loss: 1.6303 - val_accuracy: 0.3908\n",
            "Epoch 156/170\n",
            "168/168 [==============================] - 472s 3s/step - loss: 0.7698 - accuracy: 0.6916 - val_loss: 1.6389 - val_accuracy: 0.3956\n",
            "Epoch 157/170\n",
            "168/168 [==============================] - 465s 3s/step - loss: 0.7757 - accuracy: 0.6802 - val_loss: 1.6533 - val_accuracy: 0.4023\n",
            "Epoch 158/170\n",
            "168/168 [==============================] - 490s 3s/step - loss: 0.7964 - accuracy: 0.6810 - val_loss: 1.6077 - val_accuracy: 0.3899\n",
            "Epoch 159/170\n",
            "168/168 [==============================] - 466s 3s/step - loss: 0.7687 - accuracy: 0.6916 - val_loss: 1.6157 - val_accuracy: 0.3756\n",
            "Epoch 160/170\n",
            "168/168 [==============================] - 470s 3s/step - loss: 0.7785 - accuracy: 0.6865 - val_loss: 1.6337 - val_accuracy: 0.3842\n",
            "Epoch 161/170\n",
            "168/168 [==============================] - 466s 3s/step - loss: 0.7643 - accuracy: 0.6888 - val_loss: 1.6328 - val_accuracy: 0.3985\n",
            "Epoch 162/170\n",
            "168/168 [==============================] - 462s 3s/step - loss: 0.7671 - accuracy: 0.6929 - val_loss: 1.5891 - val_accuracy: 0.3908\n",
            "Epoch 163/170\n",
            "168/168 [==============================] - 498s 3s/step - loss: 0.7650 - accuracy: 0.6832 - val_loss: 1.6331 - val_accuracy: 0.4061\n",
            "Epoch 164/170\n",
            "168/168 [==============================] - 462s 3s/step - loss: 0.7563 - accuracy: 0.6918 - val_loss: 1.6801 - val_accuracy: 0.4004\n",
            "Epoch 165/170\n",
            "168/168 [==============================] - 463s 3s/step - loss: 0.7660 - accuracy: 0.6817 - val_loss: 1.6663 - val_accuracy: 0.3613\n",
            "Epoch 166/170\n",
            "168/168 [==============================] - 467s 3s/step - loss: 0.7970 - accuracy: 0.6701 - val_loss: 1.6367 - val_accuracy: 0.4013\n",
            "Epoch 167/170\n",
            "168/168 [==============================] - 460s 3s/step - loss: 0.7597 - accuracy: 0.6839 - val_loss: 1.6351 - val_accuracy: 0.3775\n",
            "Epoch 168/170\n",
            "168/168 [==============================] - 491s 3s/step - loss: 0.7657 - accuracy: 0.6888 - val_loss: 1.6532 - val_accuracy: 0.3794\n",
            "Epoch 169/170\n",
            "168/168 [==============================] - 466s 3s/step - loss: 0.7508 - accuracy: 0.6927 - val_loss: 1.6442 - val_accuracy: 0.3823\n",
            "Epoch 170/170\n",
            "168/168 [==============================] - 469s 3s/step - loss: 0.7724 - accuracy: 0.6862 - val_loss: 1.6367 - val_accuracy: 0.4109\n",
            "33/33 [==============================] - 66s 2s/step\n",
            "Validation Data Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.36      0.44      0.39       273\n",
            "       happy       0.45      0.56      0.50       303\n",
            "       panic       0.56      0.37      0.45       257\n",
            "     sadness       0.27      0.20      0.23       216\n",
            "\n",
            "    accuracy                           0.41      1049\n",
            "   macro avg       0.41      0.40      0.39      1049\n",
            "weighted avg       0.42      0.41      0.41      1049\n",
            "\n",
            "[[120  78  34  41]\n",
            " [ 61 171  22  49]\n",
            " [ 83  50  96  28]\n",
            " [ 73  81  18  44]]\n",
            "32/32 [==============================] - 785s 25s/step\n",
            "Test Data Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       0.33      0.36      0.34       280\n",
            "       happy       0.46      0.61      0.52       298\n",
            "       panic       0.57      0.38      0.46       275\n",
            "     sadness       0.29      0.23      0.25       160\n",
            "\n",
            "    accuracy                           0.42      1013\n",
            "   macro avg       0.41      0.39      0.39      1013\n",
            "weighted avg       0.43      0.42      0.41      1013\n",
            "\n",
            "[[101  99  43  37]\n",
            " [ 72 182  21  23]\n",
            " [ 86  58 104  27]\n",
            " [ 49  61  14  36]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1z6KsSpOW5BiMQn1tR38eCqAB3ecsAgU0",
      "authorship_tag": "ABX9TyO91VdXBJqbtKkPSJ8d56g/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}